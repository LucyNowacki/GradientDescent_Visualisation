{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manipualte plots you have to install ipywidgets. For details go to https://ipywidgets.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!pip install jupyterthemes\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    path_dataset = \"height_weight_genders.csv\"\n",
    "    data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    gender = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "                           converters={0:lambda x: 0 if b\"Male\" in x else 1})\n",
    "    height = data[:, 0]\n",
    "    weight = data[:, 1]\n",
    "    # Convert to metric system\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "        # sub-sample\n",
    "    if sub_sample:\n",
    "        height = height[::50]\n",
    "        weight = weight[::50]\n",
    "    if add_outlier:\n",
    "        # outlier experiment\n",
    "        height = np.concatenate([height, [1.1, 1.2]])\n",
    "        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])\n",
    "        \n",
    "    return height, weight, gender\n",
    "\n",
    "def standardise(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def build_model_data(height, weight):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardise(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    loss = np.zeros((len(w0), len(w1)))\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(w0):\n",
    "        for ind_col, col in enumerate(w1):\n",
    "            w = np.array([row, col])\n",
    "            loss[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_w(num_intervals):\n",
    "    \"\"\"Generate a grid of values for w0 and w1.\"\"\"\n",
    "    w0 = np.linspace(-25, 150, num_intervals)\n",
    "    w1 = np.linspace(-50, 50, num_intervals)\n",
    "    return w0, w1\n",
    "\n",
    "def get_best_parameters(w0, w1, losses):\n",
    "    \"\"\"Get the best w from the result of grid search.\"\"\"\n",
    "    min_row, min_col = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    return losses[min_row, min_col], w0[min_row], w1[min_col]\n",
    "\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "def base_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Base Visualization for both models.\"\"\"\n",
    "    w0, w1 = np.meshgrid(w0_list, w1_list)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # plot contourf\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    cp = ax1.contourf(w0, w1, grid_losses.T, cmap=plt.cm.jet)\n",
    "    fig.colorbar(cp, ax=ax1)\n",
    "    ax1.set_xlabel(r'$w_0$')\n",
    "    ax1.set_ylabel(r'$w_1$')\n",
    "    # put a marker at the minimum\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    ax1.plot(w0_star, w1_star, marker='*', color='r', markersize=20)\n",
    "\n",
    "    # plot f(x)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "    ax2.grid()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def grid_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Visualize how the trained model looks like under the grid search.\"\"\"\n",
    "    fig = base_visualisation(\n",
    "        grid_losses, w0_list, w1_list, mean_x, std_x, height, weight)\n",
    "\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    # plot prediciton\n",
    "    x, f = prediction(w0_star, w1_star, mean_x, std_x)\n",
    "    ax2 = fig.get_axes()[2]\n",
    "    ax2.plot(x, f, 'r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=250)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An element is called a local miminum if $\\exists$ $\\hat{w}$ s.t. \n",
    "$E(\\hat{w}) \\leq E(w)\\,\\,\\,\\,\\forall\\, w\\,\\,\\,\\,with\\,\\,\\, ||w-\\hat{w}||\\leq\\epsilon $.\n",
    "\n",
    "An element is called a global minimum point if \n",
    "$E(\\hat{w}) \\leq E(w)\\,\\,\\,\\,\\forall\\,w\\in \\mathbb{R}^n $.\n",
    "For exmaple, for regression we conclude that\n",
    "$$\\hat{w}=(X^T X)^{-1}X^Ty$$\n",
    "satisfies\n",
    "$\\hat{w}=arg\\,\\,\\underset{w}{\\min } MSE\\,(w)$ for $MSE\\,(w)=\\frac{1}{2s}||Xw-y||^2$\n",
    "\n",
    "If MSE is convex, then\n",
    "$\\Delta MSE\\,(\\hat{w})=0$ which results in $MSE\\,(\\hat{w})\\leq MSE\\,(w)\\,\\,\\,\\,\\,\\forall\\,w\\in \\mathbb{R}^n$.\n",
    "However, in case of problems without closed-form solutions , we need to use systematic approaches such as GD.\n",
    "\n",
    "Smooth functions allow the appliction of more sysytematic search compared to the grid search\n",
    "$$E\\in C^1 (\\mathbb{R}^n)\\,\\,\\,\\,\\,\\,\\Longrightarrow \\Delta E \\,\\, exists$$\n",
    "Example of smooth optimisiation is GD:\n",
    "$$w^{k+1}=w^k-\\tau\\Delta E(w^k)$$\n",
    "for some $w_0\\in \\mathbb{R}^n$ and constant $\\tau > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD for one-parameter MSE-model:    $\\,\\,\\,\\,\\,\\,\\text{MSE} w=\\frac{1}{2s}{\\sum _{j=1}^s \\left| w-y_j\\right| {}^2}$<br>\n",
    "Gradient:      $\\,\\,\\,\\,\\,\\Delta MSE(w)-\\frac{1}{s} \\sum _{j=1}^s y_j $<br>\n",
    "GD: $\\,\\,\\,\\,\\,\\,w^{k+1}=(1-\\tau)w^k+\\frac{\\tau}{s} \\sum _{j=1}^s y_j $\n",
    "\n",
    "#### For the general linear MSE: $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,MSE(w)=\\frac{1}{2s}\\left| Xw-y \\right|^2$<br>\n",
    "Recall: $\\,\\,\\,\\Delta MSE(w)=\\frac{1}{s}X^T(Xw-y)$<br>\n",
    "GD: $\\,\\,\\,\\,w^{k+1}=w^k+\\frac{\\tau}{s}X^T(y-Xw^k)=\\left(I-\\frac{\\tau}{s}X^T X \\right) y+\\frac{\\tau}{s}X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/79): loss=2792.2367127591674, w0=7.3293922002105205, w1=1.347971243498896\n",
      "Gradient Descent(1/79): loss=2264.635056030003, w0=13.925845180399996, w1=2.5611453626479035\n",
      "Gradient Descent(2/79): loss=1837.2777140793794, w0=19.862652862570513, w1=3.6530020698820147\n",
      "Gradient Descent(3/79): loss=1491.118267099375, w0=25.205779776523983, w1=4.63567310639271\n",
      "Gradient Descent(4/79): loss=1210.7291150455712, w0=30.0145939990821, w1=5.520077039252342\n",
      "Gradient Descent(5/79): loss=983.6139018819906, w0=34.3425267993844, w1=6.316040578826005\n",
      "Gradient Descent(6/79): loss=799.6505792194903, w0=38.23766631965648, w1=7.032407764442302\n",
      "Gradient Descent(7/79): loss=650.6402878628647, w0=41.74329188790136, w1=7.677138231496974\n",
      "Gradient Descent(8/79): loss=529.9419518639979, w0=44.89835489932174, w1=8.257395651846178\n",
      "Gradient Descent(9/79): loss=432.176299704916, w0=47.737911609600076, w1=8.779627330160464\n",
      "Gradient Descent(10/79): loss=352.9861214560597, w0=50.29351264885059, w1=9.24963584064332\n",
      "Gradient Descent(11/79): loss=288.842077074486, w0=52.59355358417605, w1=9.67264350007789\n",
      "Gradient Descent(12/79): loss=236.88540112541122, w0=54.66359042596896, w1=10.053350393569003\n",
      "Gradient Descent(13/79): loss=194.80049360666067, w0=56.526623583582584, w1=10.395986597711007\n",
      "Gradient Descent(14/79): loss=160.71171851647273, w0=58.20335342543484, w1=10.704359181438809\n",
      "Gradient Descent(15/79): loss=133.09981069342055, w0=59.712410283101875, w1=10.98189450679383\n",
      "Gradient Descent(16/79): loss=110.73416535674828, w0=61.070561455002206, w1=11.231676299613351\n",
      "Gradient Descent(17/79): loss=92.61799263404367, w0=62.2928975097125, w1=11.45647991315092\n",
      "Gradient Descent(18/79): loss=77.943892728653, w0=63.39299995895177, w1=11.658803165334731\n",
      "Gradient Descent(19/79): loss=66.05787180528654, w0=64.38309216326711, w1=11.84089409230016\n",
      "Gradient Descent(20/79): loss=56.430194857359716, w0=65.27417514715091, w1=12.00477592656905\n",
      "Gradient Descent(21/79): loss=48.63177652953898, w0=66.07614983264634, w1=12.152269577411047\n",
      "Gradient Descent(22/79): loss=42.315057684004195, w0=66.79792704959222, w1=12.285013863168846\n",
      "Gradient Descent(23/79): loss=37.19851541912101, w0=67.44752654484351, w1=12.404483720350866\n",
      "Gradient Descent(24/79): loss=33.054116184565615, w0=68.03216609056967, w1=12.512006591814684\n",
      "Gradient Descent(25/79): loss=29.69715280457577, w0=68.55834168172322, w1=12.60877717613212\n",
      "Gradient Descent(26/79): loss=26.978012466783984, w0=69.03189971376142, w1=12.695870702017812\n",
      "Gradient Descent(27/79): loss=24.775508793172605, w0=69.4581019425958, w1=12.774254875314936\n",
      "Gradient Descent(28/79): loss=22.99148081754737, w0=69.84168394854674, w1=12.844800631282347\n",
      "Gradient Descent(29/79): loss=21.546418157290944, w0=70.18690775390259, w1=12.908291811653017\n",
      "Gradient Descent(30/79): loss=20.375917402483257, w0=70.49760917872285, w1=12.965433873986619\n",
      "Gradient Descent(31/79): loss=19.42781179108901, w0=70.77724046106108, w1=13.016861730086863\n",
      "Gradient Descent(32/79): loss=18.659846245859693, w0=71.02890861516549, w1=13.063146800577082\n",
      "Gradient Descent(33/79): loss=18.037794154223942, w0=71.25540995385946, w1=13.104803364018277\n",
      "Gradient Descent(34/79): loss=17.53393195999899, w0=71.45926115868403, w1=13.142294271115354\n",
      "Gradient Descent(35/79): loss=17.125803582676767, w0=71.64272724302614, w1=13.176036087502723\n",
      "Gradient Descent(36/79): loss=16.795219597045776, w0=71.80784671893404, w1=13.206403722251356\n",
      "Gradient Descent(37/79): loss=16.527446568684663, w0=71.95645424725116, w1=13.233734593525124\n",
      "Gradient Descent(38/79): loss=16.310550415712164, w0=72.09020102273657, w1=13.258332377671517\n",
      "Gradient Descent(39/79): loss=16.134864531804435, w0=72.21057312067343, w1=13.28047038340327\n",
      "Gradient Descent(40/79): loss=15.992558965839178, w0=72.3189080088166, w1=13.300394588561847\n",
      "Gradient Descent(41/79): loss=15.877291457407319, w0=72.41640940814547, w1=13.318326373204567\n",
      "Gradient Descent(42/79): loss=15.783924775577509, w0=72.50416066754144, w1=13.334464979383014\n",
      "Gradient Descent(43/79): loss=15.70829776329537, w0=72.58313680099782, w1=13.348989724943618\n",
      "Gradient Descent(44/79): loss=15.647039883346832, w0=72.65421532110855, w1=13.36206199594816\n",
      "Gradient Descent(45/79): loss=15.597421000588525, w0=72.71818598920821, w1=13.373827039852248\n",
      "Gradient Descent(46/79): loss=15.557229705554294, w0=72.77575959049791, w1=13.384415579365928\n",
      "Gradient Descent(47/79): loss=15.52467475657656, w0=72.82757583165863, w1=13.39394526492824\n",
      "Gradient Descent(48/79): loss=15.498305247904602, w0=72.8742104487033, w1=13.40252198193432\n",
      "Gradient Descent(49/79): loss=15.476945945880312, w0=72.91618160404349, w1=13.410241027239794\n",
      "Gradient Descent(50/79): loss=15.459644911240638, w0=72.95395564384965, w1=13.41718816801472\n",
      "Gradient Descent(51/79): loss=15.445631073182508, w0=72.98795227967521, w1=13.423440594712153\n",
      "Gradient Descent(52/79): loss=15.434279864355416, w0=73.01854925191822, w1=13.429067778739842\n",
      "Gradient Descent(53/79): loss=15.425085385205469, w0=73.04608652693692, w1=13.434132244364763\n",
      "Gradient Descent(54/79): loss=15.417637857094014, w0=73.07087007445375, w1=13.438690263427192\n",
      "Gradient Descent(55/79): loss=15.411605359323737, w0=73.0931752672189, w1=13.442792480583378\n",
      "Gradient Descent(56/79): loss=15.406719036129816, w0=73.11324994070752, w1=13.446484476023945\n",
      "Gradient Descent(57/79): loss=15.402761114342734, w0=73.13131714684728, w1=13.449807271920456\n",
      "Gradient Descent(58/79): loss=15.399555197695204, w0=73.14757763237307, w1=13.452797788227315\n",
      "Gradient Descent(59/79): loss=15.3969584052107, w0=73.16221206934628, w1=13.455489252903488\n",
      "Gradient Descent(60/79): loss=15.394855003298254, w0=73.17538306262217, w1=13.457911571112044\n",
      "Gradient Descent(61/79): loss=15.393151247749168, w0=73.18723695657047, w1=13.460091657499744\n",
      "Gradient Descent(62/79): loss=15.391771205754415, w0=73.19790546112394, w1=13.462053735248674\n",
      "Gradient Descent(63/79): loss=15.390653371738663, w0=73.20750711522206, w1=13.463819605222712\n",
      "Gradient Descent(64/79): loss=15.389747926185903, w0=73.21614860391037, w1=13.465408888199345\n",
      "Gradient Descent(65/79): loss=15.389014515288169, w0=73.22392594372985, w1=13.466839242878315\n",
      "Gradient Descent(66/79): loss=15.388420452461002, w0=73.23092554956739, w1=13.468126562089388\n",
      "Gradient Descent(67/79): loss=15.387939261570997, w0=73.23722519482116, w1=13.469285149379354\n",
      "Gradient Descent(68/79): loss=15.387549496950095, w0=73.24289487554957, w1=13.470327877940324\n",
      "Gradient Descent(69/79): loss=15.387233787607164, w0=73.24799758820512, w1=13.471266333645197\n",
      "Gradient Descent(70/79): loss=15.386978063039388, w0=73.25259002959513, w1=13.472110943779581\n",
      "Gradient Descent(71/79): loss=15.386770926139489, w0=73.25672322684613, w1=13.472871092900528\n",
      "Gradient Descent(72/79): loss=15.386603145250573, w0=73.26044310437204, w1=13.47355522710938\n",
      "Gradient Descent(73/79): loss=15.386467242730548, w0=73.26379099414535, w1=13.474170947897347\n",
      "Gradient Descent(74/79): loss=15.38635716168933, w0=73.26680409494134, w1=13.474725096606518\n",
      "Gradient Descent(75/79): loss=15.386267996045945, w0=73.26951588565773, w1=13.47522383044477\n",
      "Gradient Descent(76/79): loss=15.386195771874801, w0=73.27195649730247, w1=13.475672690899199\n",
      "Gradient Descent(77/79): loss=15.386137270296176, w0=73.27415304778275, w1=13.476076665308184\n",
      "Gradient Descent(78/79): loss=15.386089884017489, w0=73.276129943215, w1=13.47644024227627\n",
      "Gradient Descent(79/79): loss=15.38605150113175, w0=73.27790914910402, w1=13.476767461547547\n",
      "Gradient Descent: execution time=0.032 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 80\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b44d27bc5e4fffba83c8f8cbdbaf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=81, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from plots import gradient_descent_visualisation\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(14.0, 20.0)\n",
    "    \n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/79): loss=2526.9988547234075, w0=5.358881890041438, w1=-6.720690003612224\n",
      "SGD(1/79): loss=1874.0700283597848, w0=13.950476723701799, w1=-0.5104192109625263\n",
      "SGD(2/79): loss=1390.8539615234013, w0=21.262646149959274, w1=6.870440122015251\n",
      "SGD(3/79): loss=1155.3492891247627, w0=25.927992300280614, w1=7.446843632033031\n",
      "SGD(4/79): loss=892.7942250472892, w0=32.02858457812828, w1=6.269400261285946\n",
      "SGD(5/79): loss=691.5128086062932, w0=36.605723330445365, w1=10.983731725178554\n",
      "SGD(6/79): loss=548.7208508001407, w0=40.64463477011975, w1=14.312760516395663\n",
      "SGD(7/79): loss=447.21124163204496, w0=44.21165729087483, w1=17.707310664759277\n",
      "SGD(8/79): loss=366.7186227680209, w0=46.919885532414106, w1=16.139725610480387\n",
      "SGD(9/79): loss=285.89681103964074, w0=50.16358597023508, w1=15.931120478215231\n",
      "SGD(10/79): loss=231.41475663730483, w0=52.51606070097061, w1=14.061276952274078\n",
      "SGD(11/79): loss=192.3552008572033, w0=54.865603297296296, w1=17.26596331029279\n",
      "SGD(12/79): loss=150.37929737301866, w0=57.412881726071824, w1=17.69627249733661\n",
      "SGD(13/79): loss=112.26134719089673, w0=59.60083090331108, w1=15.97974739559661\n",
      "SGD(14/79): loss=87.99337147029948, w0=61.7758882862934, w1=17.022291513056743\n",
      "SGD(15/79): loss=76.5269543251354, w0=62.84869032887113, w1=17.110038619380735\n",
      "SGD(16/79): loss=68.91867512948953, w0=64.0886578389646, w1=18.205036361252688\n",
      "SGD(17/79): loss=59.481620058834835, w0=64.7708146371759, w1=17.422821008931557\n",
      "SGD(18/79): loss=49.928601280798404, w0=66.39385741728105, w1=18.113774966849583\n",
      "SGD(19/79): loss=47.06012377820894, w0=66.6895639585576, w1=17.921663205921483\n",
      "SGD(20/79): loss=44.42924506098717, w0=67.54902819455208, w1=18.48799652479999\n",
      "SGD(21/79): loss=44.403657059584724, w0=67.88541850260533, w1=18.844750018958838\n",
      "SGD(22/79): loss=44.978452110777546, w0=67.76612569874283, w1=18.830282172169344\n",
      "SGD(23/79): loss=41.422172182740944, w0=68.15206908216719, w1=18.542707317225062\n",
      "SGD(24/79): loss=39.86772307285586, w0=68.90363980560448, w1=18.928482263412874\n",
      "SGD(25/79): loss=34.58451973690433, w0=69.78369129497752, w1=18.58613428584389\n",
      "SGD(26/79): loss=38.96653011057168, w0=68.42390033656015, w1=18.32163080074104\n",
      "SGD(27/79): loss=31.02782559978984, w0=69.09194631141766, w1=17.17122645457395\n",
      "SGD(28/79): loss=29.51022129060244, w0=69.61826833542688, w1=17.31875358691492\n",
      "SGD(29/79): loss=31.19997428875613, w0=68.0898494822488, w1=15.611801093541725\n",
      "SGD(30/79): loss=30.24973801578561, w0=68.22380168056912, w1=15.48510023266296\n",
      "SGD(31/79): loss=32.688418790013884, w0=67.49355287364743, w1=14.459906199508671\n",
      "SGD(32/79): loss=32.80018758928131, w0=67.4785513989983, w1=14.484731932025841\n",
      "SGD(33/79): loss=29.643648287871393, w0=67.9730431112314, w1=13.931119863746861\n",
      "SGD(34/79): loss=27.82004284548842, w0=68.31024991157187, w1=13.656693923731007\n",
      "SGD(35/79): loss=26.893783536092, w0=68.51089691631442, w1=13.85181774699281\n",
      "SGD(36/79): loss=27.839348948549475, w0=68.33319737492567, w1=14.02572830722612\n",
      "SGD(37/79): loss=25.20394154084071, w0=69.34616854446637, w1=15.492508992449169\n",
      "SGD(38/79): loss=27.515328611245767, w0=68.84679303580943, w1=15.59676827789114\n",
      "SGD(39/79): loss=22.002956743291097, w0=69.8762602152839, w1=14.726197757736552\n",
      "SGD(40/79): loss=18.49941132981403, w0=70.79940959002792, w1=13.412968536678418\n",
      "SGD(41/79): loss=18.811616617720095, w0=70.67943067068745, w1=13.35364667547087\n",
      "SGD(42/79): loss=16.67770380026636, w0=71.7343267766184, w1=13.868678494578749\n",
      "SGD(43/79): loss=16.07926823492449, w0=72.11636202109696, w1=13.490353079616318\n",
      "SGD(44/79): loss=15.512223735932093, w0=73.61024849831846, w1=13.089060054990123\n",
      "SGD(45/79): loss=15.723389300691343, w0=73.95420598639807, w1=12.99080761418012\n",
      "SGD(46/79): loss=16.02949845352363, w0=74.19125066827286, w1=12.785434079959733\n",
      "SGD(47/79): loss=15.788422358974177, w0=73.97896791150752, w1=12.900246230365623\n",
      "SGD(48/79): loss=16.089356024572997, w0=73.61942110589797, w1=12.339104419535321\n",
      "SGD(49/79): loss=16.75614656909323, w0=74.13173155173665, w1=12.051919537165984\n",
      "SGD(50/79): loss=16.7120422842082, w0=74.11295446636836, w1=12.072056705330484\n",
      "SGD(51/79): loss=16.847625218761657, w0=73.2049961589991, w1=11.772209318068795\n",
      "SGD(52/79): loss=16.578482815166808, w0=73.39453112958407, w1=11.938586970874347\n",
      "SGD(53/79): loss=16.627912414666326, w0=73.77591475455809, w1=11.979135186642022\n",
      "SGD(54/79): loss=16.623806809281128, w0=73.59878243813193, w1=11.936048612957505\n",
      "SGD(55/79): loss=16.11768788026605, w0=74.18621367433171, w1=12.662757360001141\n",
      "SGD(56/79): loss=16.352758321203023, w0=74.39779409285619, w1=12.634013131448917\n",
      "SGD(57/79): loss=16.406230926063778, w0=74.47654812052443, w1=12.678412504562617\n",
      "SGD(58/79): loss=15.84756495858531, w0=74.07212222483858, w1=12.916011634165141\n",
      "SGD(59/79): loss=15.422182266010926, w0=73.25971774560195, w1=13.746955513851939\n",
      "SGD(60/79): loss=15.445436196052587, w0=73.4889515647928, w1=13.195001837848285\n",
      "SGD(61/79): loss=15.589025064257198, w0=73.86373762611386, w1=13.194082349929364\n",
      "SGD(62/79): loss=15.606669605299537, w0=73.92608956915777, w1=13.274950035835545\n",
      "SGD(63/79): loss=15.395840554820438, w0=73.43432130986089, w1=13.493619496641243\n",
      "SGD(64/79): loss=15.909114062325017, w0=74.16523473166654, w1=12.943739941016107\n",
      "SGD(65/79): loss=15.532074361922021, w0=73.798172472393, w1=13.284508824879605\n",
      "SGD(66/79): loss=15.572474506689518, w0=73.45901437588421, w1=14.067860191798715\n",
      "SGD(67/79): loss=15.617587822427954, w0=73.50687247032727, w1=14.126281847566773\n",
      "SGD(68/79): loss=15.392961817869448, w0=73.19329256511651, w1=13.543128635533034\n",
      "SGD(69/79): loss=15.794580315431661, w0=73.78620572986665, w1=14.238027425336597\n",
      "SGD(70/79): loss=15.77125646120396, w0=73.66958175839997, w1=14.273196481614857\n",
      "SGD(71/79): loss=15.864562195949393, w0=73.57168212165783, w1=14.417901099504927\n",
      "SGD(72/79): loss=15.776150682249828, w0=73.50753990595507, w1=14.33697144952062\n",
      "SGD(73/79): loss=15.475353761451085, w0=72.87382520656202, w1=13.430210236878953\n",
      "SGD(74/79): loss=15.591570137005878, w0=72.68110703513362, w1=13.29044449631852\n",
      "SGD(75/79): loss=15.863891498070192, w0=72.33373930246921, w1=13.664256309992673\n",
      "SGD(76/79): loss=15.48153408834713, w0=73.20015607157764, w1=13.906913077904958\n",
      "SGD(77/79): loss=15.487317205659634, w0=72.94783343338852, w1=13.767950836774999\n",
      "SGD(78/79): loss=15.62752965956114, w0=72.60998179133878, w1=13.60424905434369\n",
      "SGD(79/79): loss=16.362101487999492, w0=71.93303742810485, w1=13.796604237641175\n",
      "SGD: execution time=0.080 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 80\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db805b6107b44e9caa033c6162bd31ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=81, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from plots import gradient_descent_visualisation\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(14.0, 20.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "tor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
